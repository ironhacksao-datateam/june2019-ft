{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"DADOS-AUSENTES:\">DADOS AUSENTES:<a class=\"anchor-link\" href=\"#DADOS-AUSENTES:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Nessa seção vamos aprender como lidar com tabelas incompletas. Por padrão o pandas reconhece apenas alguns caracteres como valores ausentes (que seria 'NA', 'NaN' ou simplesmente um vazio).</p>\n",
    "<p>Mas as vezes um processo pode marcar um valor ausente com algum caracter específico e o pandas consegue trabalhar muito bem nessas situações bastante apenas utilizar a option 'na_values' da ação de leitura '.read_csv()' como no exemplo abaixo:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('/Users/julianaforlin/Downloads/vehicles/vehicles_messy.csv',low_memory=False)\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Se ainda assim existir alguma coluna que tenha algum caso bem específico como, por exemplo, uma determinada variável só pode ter letras mas por algum motivo veio um número inteiro no meio. Pode realizar o seguinte comando:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detectando se há algum inteiro e transformando em missing através da função do Numpy np.nan \n",
    "cnt=0\n",
    "for row in data['mfrCode']:\n",
    "    try:\n",
    "        int(row)\n",
    "        data.loc[cnt, 'mfrCode']=np.nan\n",
    "    except ValueError:\n",
    "        pass\n",
    "    cnt+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>o comando try e except funcionam da seguinte maneira:\n",
    "o try tentará rodar algum bloco de comando, caso aconteça algum erro, ele roda o bloco de comando do except</p>\n",
    "<p>nesse caso, estou olhando uma variável que tem apenas letras e estou vendo se algum elemento dela eu consigo transformar em inteiro (o que nesse caso será transformado em NaN). Caso ele não consiga transformar em inteiro, será retornado um erro e o processo seguirá para o except, aonde o comando irá continuar o processo do for normalmente, sem parar por causa do erro,</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Perceba que algumas células do nosso dataframe contém <b>NaN</b>, que significa <i>not a number</i>, o que é apenas uma forma\n",
    "de dizer que a não se tem aquela informação, o dado está ausente.</p>\n",
    "<p>A função <i>isnull()</i> retorna um df de variáveis booleanas indicando se a informação está ausente</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.isnull().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para saber a quantidade de células com Nan em cada uma das colunas, podemos fazer:\n",
    "data.isnull().sum()\n",
    "\n",
    "# Para verificar quantas colunas possuem pelo menos um elemento NaN\n",
    "len(data.loc[:,data.isnull().sum() > 0].columns)\n",
    "\n",
    "# Para saber a quantidade de células com NaN apenas nas colunas com NaN:\n",
    "data.loc[:,data.isnull().sum() > 0].isnull().sum()\n",
    "\n",
    "# esta primeira parte df.loc[:,df.isnull().sum() > 0] está filtrando a base,\n",
    "# selecionando todas as linhas (com o comando : no primeiro espaço entre os colchetes) das colunas com pelo menos um NaN\n",
    "# A segunda parte .isnull().sum() realiza a contagem de quantos NaN tem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para saber a porcentagem de células com Nan em cada uma das colunas, podemos fazer:\n",
    "data.isnull().sum()/data.shape[0]\n",
    "\n",
    "# Para verificar quantas colunas possuem pelo menos 80% dos valores como um elemento NaN\n",
    "len(data.loc[:,data.isnull().sum()/data.shape[0] > 0.8].columns)\n",
    "\n",
    "# Para saber a quantidade de células com NaN apenas nas colunas com pelo menos 80% de NaN:\n",
    "data.loc[:,data.isnull().sum()/data.shape[0] > 0.8].isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Só para facilitar a visualização dos dados ausentes - para saber mais leia o tutorial 'Seaborn'\n",
    "# Em preto, os dados ausentes\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(data.isnull(), cbar=False, yticklabels=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>E agora? Temos algumas colunas com falta de informações. \n",
    "O que fazer? Como resolver esse problema?</p>\n",
    "<p>Não existe uma maneira correta ou única para resolver isso, vai depender de cada caso. Algumas opções são:</p>\n",
    "<p>a) Apagar a coluna;    b) Apagar a linha;   c) Preencher com algum valor</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vamos resolver o problema da guzzler. Nessa coluna temos 35562 valores nulos. Vimos que nosso dataframe tem 37843 linhas,\n",
    "# ou seja, aqui não dá pra fazer muita coisa, melhor apagar toda coluna:\n",
    "data['guzzler2'] = data['guzzler']\n",
    "data.drop('guzzler2', axis=1, inplace = True) # axis é o eixo: 0 p linha e 1 p coluna\n",
    "                                         # lembre sempre do inplace = True, para que a mudança seja permanente no nosso df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mas ainda temos muito missings, podemos remover direto as colunas com mais de 10000 valores missings\n",
    "null_cols = data.isnull().sum()\n",
    "null_cols[null_cols > 0]\n",
    "\n",
    "drop_cols = list(null_cols[null_cols > 10000].index)\n",
    "data = data.drop(drop_cols, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agora vamos para a coluna de 'cylinders', uma opção aqui seria preencher os valores ausentes com a média dela\n",
    "\n",
    "data['cylinders'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uma solução mais elegante e mais correta seria pegar a média de cada classe de 'Model' para preencher os\n",
    "# valores, posso fazer isso com o groupby\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "data.groupby('VClass')['cylinders'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agora é só criar uma função para preencher os valores que faltam com as médias calculadas:\n",
    "\n",
    "def preenchedor(cols):   # a função será aplicada em uma lista de colunas\n",
    "    cylinders = cols[0]     # sendo que a primeira é a da cylinders, index =[0]\n",
    "    VClass       = cols[1]     # e a segunda é a da RAT_BACEN, index = [1]\n",
    "    \n",
    "    if pd.isnull(cylinders):     # se o valor na coluna cylinders for NaN\n",
    "        if VClass   == 'Compact Cars':      # e se pertencer a classe \"Compact Cars\" de VClass\n",
    "            return 4.82        # retorna a média de cylinders da classe \"Compact Cars\"  = 4.82\n",
    "        elif VClass == 'Large Cars':\n",
    "            return 7.13\n",
    "        elif VClass == 'Midsize Cars':\n",
    "            return 5.68  \n",
    "        elif VClass == 'Midsize Station Wagons':\n",
    "            return 5.12  \n",
    "        elif VClass == 'Midsize-Large Station Wagons':\n",
    "            return 5.18\n",
    "        elif VClass == 'Minicompact Cars':\n",
    "            return 5.76\n",
    "        elif VClass == 'Minivan - 2WD':\n",
    "            return 5.80 \n",
    "        else:\n",
    "            return 0            \n",
    "    else:                                  # mas se o valor de cylinders não for Nan  \n",
    "        return cylinders             # recebe o próprio valor\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicando a função para modificar a coluna Age\n",
    "\n",
    "data['cylinders_2'] = data[['cylinders','VClass']].apply(preenchedor, axis=1)\n",
    "print(\"Variável original:  \", data['cylinders'].isnull().sum())\n",
    "print(\"VAriável modificada:\", data['cylinders_2'].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(data.isnull(), cbar=False, yticklabels=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Baixa variância:-\"><u>Baixa Variância: </u><a class=\"anchor-link\" href=\"#Baixa Variância:-\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance = []\n",
    "\n",
    "for col in data._get_numeric_data():\n",
    "    minimum = min(data[col])\n",
    "    ninety_perc = np.percentile(data[col], 90)\n",
    "    if ninety_perc == minimum:\n",
    "        low_variance.append(col)\n",
    "\n",
    "print(low_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(low_variance, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Outliers:-\"><u>Outliers: </u><a class=\"anchor-link\" href=\"#Outliers:-\">¶</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = data.describe().transpose()\n",
    "stats['IQR'] = stats['75%'] - stats['25%']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "for col in stats.index:\n",
    "    iqr = stats.at[col,'IQR']\n",
    "    cutoff = iqr * 1.5\n",
    "    lower = stats.at[col,'25%'] - cutoff\n",
    "    upper = stats.at[col,'75%'] + cutoff\n",
    "    results = data[(data[col] < lower) | \n",
    "                   (data[col] > upper)].copy()\n",
    "    results['Outlier'] = col\n",
    "    outliers = outliers.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"JUNTAR-DATAFRAMES\">JUNTAR DATAFRAMES<a class=\"anchor-link\" href=\"#JUNTAR-DATAFRAMES\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>As três principais funções para unir dataframes são: <i>  .concat(), .join() e .merge()</i><br/>\n",
    "Vamos ver as diferenças entre elas e quando usar cada uma.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temos 2 dataframes df1 e df2\n",
    "\n",
    "df1 = pd.DataFrame(columns= 'A B C'.split(), index= [0,1,2], data=[['a1','b1','c1'],\n",
    "                                                                   ['a2', 'b2', 'c2'], \n",
    "                                                                   ['a3', 'b3', 'c3']])\n",
    "df2 = pd.DataFrame(columns= 'A B C'.split(), index= [3,4,5], data=[['a3','b3','c3'],\n",
    "                                                                   ['a4', 'b4', 'c4'], \n",
    "                                                                   ['a5', 'b5', 'c5']])\n",
    "\n",
    "df3 = pd.DataFrame(columns= 'D E F'.split(), index= [0, 1, 2], data=[['d1','e1','f1'],\n",
    "                                                                   ['d2', 'e2', 'f2'], \n",
    "                                                                   ['d3', 'e3', 'f3']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b> CONCAT ( ) : </b>  basicamente cola dois ou mais dataframes juntos</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A primeira função que veremos é a 'concat'. Ela basicamente junta dois dataframes.\n",
    "\n",
    "pd.concat([df1, df2])  # passamos uma lista com os df a serem concatenados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para juntar pelo eixo das colunas, use 'axis=1'. No nosso caso, resultou em alguns valores NaN, pois os df originais\n",
    "# não possuem a informação das respectivas células\n",
    "\n",
    "pd.concat([df1, df2], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Um bom uso para o 'axis=1' seria na concatenação do df1 com o df3, pois compartilham o mesmo índice com colunas diferentes\n",
    "\n",
    "pd.concat([df1, df3], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b> JOIN( ): </b> faz a união de dataframes no índice ou em alguma coluna (transformado em índice)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suponha que você tenha 2 df com informações sobre alguns clientes\n",
    "\n",
    "func1 = data.loc[1:1000, ['cylinders','drive']]\n",
    "func2 = data.loc[1:2000, ['trany'      ,'UCity']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func1   # func1 tem dados dos clientes sobre o Cylinders e Drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func2  # func2 tem dados sobre o trany e UCity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# O parâmetro 'how' é a forma de união, são quatro opções: left, right, inner, outer \n",
    "\n",
    "func1.join(func2)   # left é o padrão: vai usar o índice do df da esquerda, no nosso caso func1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func1.join(func2, how='right')   # right usa o índice do df da direita, no nosso caso func2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func1.join(func2, how='inner')  # inner só usa os índices em comum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func1.join(func2, how='outer')  # outter usa todos os índices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Com esse tutorial rápido já dá pra fazer muita coisa em termo de união de dataframes. <br/>\n",
    "Para mais informações sobre merge, join e concat, leia a documentação: <a href=\"http://pandas.pydata.org/pandas-docs/stable/merging.html\">http://pandas.pydata.org/pandas-docs/stable/merging.html</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Aplicando-funções-e-Operações-nos-dataframes:\">Aplicando funções e Operações nos dataframes:<a class=\"anchor-link\" href=\"#Aplicando-funções-e-Operações-nos-dataframes:\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Primeiro passo é escrever uma função\n",
    "\n",
    "def dobrar(x):      # função simples que retorna o dobro do número passado\n",
    "    return x * 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agora é só aplicar na coluna desejada com .apply(), passando a nossa função como argumento\n",
    "\n",
    "data['cylinders_2'] = data['cylinders'].apply(dobrar)    # para tornar a alteração permanente, atribuímos o resultado a nossa coluna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[['cylinders_2','cylinders']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# O mesmo resultado pode ser obtido através de uma função lambda\n",
    "\n",
    "data['cylinders'].apply(lambda x: x*2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Normalização-e-Padronização-dos-Dados\">Normalização e Padronização dos Dados<a class=\"anchor-link\" href=\"#Normalização-e-Padronização-dos-Dados\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>O processo de normalização se dá quando pegamos cada elemento de uma variável e fazemos a diferença com a média e em seguida dividimos pelo desvio padrão. Para realizar esse processo pelo pandas podemos fazer da seguinte maneira:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2 = data.loc[:,data.select_dtypes(include = ['float64','int64']).columns].copy()\n",
    "normalizado_df=(data2 - data2.mean())/data2.std()\n",
    "normalizado_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_to_normalize = ['barrels08','city08','city08U', 'co2','co2TailpipeGpm', 'comb08','comb08U', 'cylinders']\n",
    "data2 = data[lista_to_normalize].copy()\n",
    "normalizado_df=(data2 - data2.mean())/data2.std()\n",
    "normalizado_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2 = data.copy()\n",
    "for coluna in lista_to_normalize:\n",
    "    data2[coluna] = (data[coluna] - data[coluna].mean()) / (data[coluna].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>O processo de padronização é quando queremos deixar todos os valores de todas as colunas numéricas entre 0 e 1. Existem diversas técnicas que não funcionam muito bem quando a amplitude das variáveis são muito distintas~, fazendo com que esse processo seja muito útil. Para fazer essa transformação devemos fazer uma subtração entre o valor da variável e o mínimo dela e depois dividir pela amplitude (máximo - mínimo):</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data3 = data.loc[:,data.select_dtypes(include = ['float64','int64']).columns].copy()\n",
    "padronizado_df=(data3 - data3.min())/(data3.max() - data3.min())\n",
    "padronizado_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_to_padronize = ['barrels08','city08','city08U', 'co2','co2TailpipeGpm', 'comb08','comb08U', 'cylinders']\n",
    "data3 = data.copy()\n",
    "for coluna in lista_to_padronize:\n",
    "    df3[coluna] = (data[coluna] - data[coluna].min()) /(data[coluna].max() - data[coluna].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Criacao-de-Dummys\">Criacao de Dummys<a class=\"anchor-link\" href=\"#Criacao-de-Dummys\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Para criação de dummys no banco de dados, podemos utilizar a função do pandas pd.get_dummies()</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.get_dummies(data = data['drive'], drop_first = True, prefix = 'drive',dtype = 'int').head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.get_dummies(data = data[['drive','trany']], drop_first = True, prefix = ['drive','trany'],dtype = 'int').head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Gráficos-com-Pandas-e-Matplotlib\">Gráficos com Pandas e Matplotlib<a class=\"anchor-link\" href=\"#Gráficos-com-Pandas-e-Matplotlib\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>O pandas possuí também uma integração com o matplotlib que facilita muito na construção de gráficos</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importar o matplotlib e chama-lo de plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Para construir um gráfico de barras podemos utilizar a ação \".plot(kind = 'bar')\" e devemos ter um objeto do formato \"Series\" que tenha categorias</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['drive'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Realizar um gráfico de barras\n",
    "data['drive'].value_counts().plot(kind='bar', color = 'black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Para fazer um scatterplot devemos realizar a seguinte ação em um pd.DataFrame.plot(x = var1, y = var2, kind = 'scatter)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[['UHighway','UCity']].plot(x ='UHighway',  y = 'UCity',   kind='scatter', color = 'black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['UHighway','UCity']].plot(x ='UHighway'\n",
    "                                             ,  y = 'UCity'\n",
    "                                             ,   kind='scatter'\n",
    "                                             , color = 'black'\n",
    "                                            ,xlim = [0,100]\n",
    "                                            ,ylim = [0,100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b>FONTES:</b></p>\n",
    "<p>1 - <a href=\"http://pandas.pydata.org/\">http://pandas.pydata.org/</a> <br/>\n",
    "2 - <a href=\"http://pandas.pydata.org/pandas-docs/stable/\">http://pandas.pydata.org/pandas-docs/stable/</a><br/>\n",
    "3 - <a href=\"http://pandas.pydata.org/pandas-docs/stable/10min.html#min\">http://pandas.pydata.org/pandas-docs/stable/10min.html#min</a><br/>\n",
    "4 - <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#general-functions\">http://pandas.pydata.org/pandas-docs/stable/api.html#general-functions</a><br/></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
