{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ironhack logo](https://i.imgur.com/1QgrNNw.png)\n",
    "\n",
    "# Lab | Data Cleaning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We keep seeing a common phrase that 80% of the work of a data scientist is data cleaning. We have no idea whether this number is accurate but a data scientist indeed spends lots of time and effort in collecting, cleaning and preparing the data for analysis. This is because datasets are usually messy and complex in nature. It is a very important ability for a data scientist to refine and restructure datasets into a usable state in order to proceed to the data analysis stage.\n",
    "\n",
    "In this exercise, you will both practice the data cleaning techniques we discussed in the lesson and learn new techniques by looking up documentations and references. You will work on your own but remember the teaching staff is at your service whenever you encounter problems.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Now you should already be familar with the workflow of solving and submitting the labs. But in case not, review the guidelines in the `README.md` in the [repo root](../..) and [previous lab](../lab-pandas).\n",
    "\n",
    "In this lab you will be working on [main.ipynb](your-code/main.ipynb). To launch it, first navigate to the directory that contains `main.ipynb` in Terminal, then execute `jupyter notebook`. In the webpage that is automatically opened, click the `main.ipynb` link to launch it.\n",
    "\n",
    "When you are on `main.ipynb`, read the instructions for each cell and provide your answers. Make sure to test your answers in each cell and save. Jupyter Notebook should automatically save your work progress. But it's a good idea to periodically save your work manually just in case.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Do you remember your MySQL project? In this lab, you will examine some MySQL tables from [here](https://relational.fit.cvut.cz/dataset/Stats). This database contains an anonymized dump of all user-contributed content on the Stats Stack Exchange network.\n",
    "\n",
    "You will need to import the `pymysql` library and the `create_engine` function from the `sqlalchemy` library.\n",
    "\n",
    "```python\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "```\n",
    "\n",
    "Once your connection is established with the database yoo will use some basic SELECT queries to retrieve the data in order to answer the questions described next.\n",
    "\n",
    ":bulb: If you receive import errors for `pymysql` or `sqlalchemy`, it means you need to install them with `pip`.\n",
    "\n",
    "### Challenge Questions\n",
    "\n",
    "1. Connect to the server and collect all the data from users and posts tables.\n",
    "\n",
    "1. Create a merged dataframe with users and post tables. **Take into account that you will need to do some stuff before merging.**\n",
    "\n",
    "1. Identify missing values in the merged dataframe and apply some of the methods.\n",
    "\n",
    "1. Change the data types of your merged dataset accordingly.\n",
    "\n",
    "1. Bonus Question: Create a dataframe with the outliers you have identified in the dataframe and export it to a csv file in your-code folder. \n",
    "\n",
    "**:exclamation: If you feel you are already good at Python/Pandas and don't need the instructions in `main.ipynb` to walk you through, please feel free to skip `main.ipynb` and create your own solution file.**\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- `main.ipynb` with your responses to each of the questions above.\n",
    "- `outliers.csv` containing the outliers of the dataset.\n",
    "- `weather.ipynb` containing the additional challenge code and results.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Upon completion, add your deliverables to git. Then commit git, push to your forked repo, and create the pull request as in the previous labs. **REMEMBER\n",
    "\n",
    "- Upon completion, commit your code and submit to github. **REMEMBER YOU HAVE ALREADY FORKED THE REPO BEFORE**!!\n",
    "\n",
    "  ```\n",
    "  git add .\n",
    "  git commit -m \"<lab or project name>\"\n",
    "  git push origin master\n",
    "  ```\n",
    "\n",
    "- Navigate to your repo and [create a Pull Request](https://help.github.com/articles/creating-a-pull-request/).\n",
    "- Create a pull request with title following this format: **\"[<your_campus>][<bootcamp_code>] [<lab/project_name>]<your_name>\"**\n",
    "  - For instance, if you are doing data bootcamp in Madrid, your name is Marc Pomar and the lab you are working on is `lab-numpy`, your pull request should be named like this: \"[MAD][datamad10108] [lab-numpy] Marc Pomar\"\n",
    "- If you have successfully created the pull request you are done!  CONGRATS :)\n",
    "\n",
    "## Resources\n",
    "\n",
    "[Data Cleaning Tutorial](https://www.tutorialspoint.com/python/python_data_cleansing.html)\n",
    "\n",
    "[Data Cleaning with Numpy and Pandas](https://realpython.com/python-data-cleaning-numpy-pandas/#python-data-cleaning-recap-and-resources)\n",
    "\n",
    "[Data Cleaning Video](https://www.youtube.com/watch?v=ZOX18HfLHGQ)\n",
    "\n",
    "[Data Preparation](https://www.kdnuggets.com/2017/06/7-steps-mastering-data-preparation-python.html)\n",
    "\n",
    "[Google Search](https://www.google.es/search?q=how+to+clean+data+with+python)\n",
    "\n",
    "## Additional Challenges for the Nerds\n",
    "\n",
    "If you have completed the `Stats` challenge without much difficulty, you can try to tidy the data you will find in thie lab folder [weather](../weather-raw.csv). This dataset is a subset of a global historical climatology network dataset. The data represents the daily weather records for a weather station (MX17004) in Mexico for five months in 2010. The goal of this additional challenge is to get the most tidy dataset you are able to produce. **Hint:Variables are stored in both rows and columns.**\n",
    "\n",
    "To accomplish this challenge, you will need to do some research on tidying and melt&pivot. Feel free to reference any resources you consider appropiate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ruamel.yaml in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (0.15.98)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# to resolve\n",
    "# ERROR: Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/zx/plpqjh0n3mv5_t81883pv0040000gn/T/pip-install-7d2fdlv1/MySQL-python/\n",
    "# https://github.com/gunthercox/ChatterBot/issues/1055\n",
    "!pip install ruamel.yaml --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (0.9.3)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for erros: ModuleNotFoundError: No module named 'MySQLdb'\n",
    "! pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysqlclient\n",
      "Installing collected packages: mysqlclient\n",
      "Successfully installed mysqlclient-1.4.2.post1\n",
      "Requirement already satisfied: mysql-connector in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (2.2.9)\n",
      "Requirement already satisfied: pymysql in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (0.9.3)\n",
      "Requirement already satisfied: mysql-connector-python-rf in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: sqlalchemy-utils in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (0.33.11)\n",
      "Requirement already satisfied: six in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (from sqlalchemy-utils) (1.12.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.0 in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (from sqlalchemy-utils) (1.2.16)\n",
      "Collecting MySQL-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/e9/51b544da85a36a68debe7a7091f068d802fc515a3a202652828c73453cad/MySQL-python-1.2.5.zip (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 131kB/s eta 0:00:01\n",
      "\u001b[31m    ERROR: Complete output from command python setup.py egg_info:\u001b[0m\n",
      "\u001b[31m    ERROR: Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/zx/plpqjh0n3mv5_t81883pv0040000gn/T/pip-install-7d2fdlv1/MySQL-python/setup.py\", line 13, in <module>\n",
      "        from setup_posix import get_config\n",
      "      File \"/private/var/folders/zx/plpqjh0n3mv5_t81883pv0040000gn/T/pip-install-7d2fdlv1/MySQL-python/setup_posix.py\", line 2, in <module>\n",
      "        from ConfigParser import SafeConfigParser\n",
      "    ModuleNotFoundError: No module named 'ConfigParser'\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/zx/plpqjh0n3mv5_t81883pv0040000gn/T/pip-install-7d2fdlv1/MySQL-python/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install --user mysqlclient \n",
    "! pip3 install mysql-connector \n",
    "! pip install mysql-connector-python-rf\n",
    "! pip install sqlalchemy-utils\n",
    "! sudo pip install MySQL-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/44/ed3d30a8eb901d7316430f73bfb620b91b56decbf021904407652e5f470d/mysql_connector_python-8.0.16-cp36-cp36m-macosx_10_13_x86_64.whl (4.2MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2MB 125kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (from mysql-connector-python) (3.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (from protobuf>=3.0.0->mysql-connector-python) (41.0.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/liviaclarete/.pyenv/versions/3.6.0/envs/dataAnalysis/lib/python3.6/site-packages (from protobuf>=3.0.0->mysql-connector-python) (1.12.0)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.0.16\n"
     ]
    }
   ],
   "source": [
    "! pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Connectors: https://docs.sqlalchemy.org/en/13/core/engines.html\n",
    "\n",
    "* MySQLdb is not supported by python3? https://docs.djangoproject.com/en/1.11/ref/databases/#mysql-db-api-drivers\n",
    "https://stackoverflow.com/questions/14164183/python-3-and-mysql-through-sqlalchemy\n",
    "\n",
    "* Data source: * https://relational.fit.cvut.cz/dataset/Accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/CoreyMSchafer/code_snippets/blob/master/Python/Flask_Blog/04-Database/flaskblog.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mysql database\n",
    "* https://relational.fit.cvut.cz/dataset/Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, import pandas using 'pd' as an alias\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import pymysql and sqlalchemy as you have learnt in the lesson of importing/exporting data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a mysql engine to set the connection to the server. Check the connection details in [this link](https://relational.fit.cvut.cz/dataset/Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define driver and dialect settings\n",
    "driver = 'mysql'\n",
    "dialect = 'pymysql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access info accordingly the link information\n",
    "db_user = 'guest'\n",
    "db_pw = 'relational'\n",
    "db_host = 'relational.fit.cvut.cz'\n",
    "db_port = 3306\n",
    "database = 'stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql format connection mask\n",
    "mysql_mask = '{0}+{1}://{2}:{3}@{4}:{5}/{6}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the string with the connect into the mysql\n",
    "mysql_info = mysql_mask.format(\n",
    "    driver,\n",
    "    dialect,\n",
    "    db_user, \n",
    "    db_pw, \n",
    "    db_host, \n",
    "    db_port, \n",
    "    database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/stats\n"
     ]
    }
   ],
   "source": [
    "print(mysql_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an engine\n",
    "mysql_engine = create_engine(mysql_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the engine to retrieve data\n",
    "mysql_conn = mysql_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['badges', 'comments', 'postHistory', 'postLinks', 'posts', 'tags', 'users', 'votes']\n"
     ]
    }
   ],
   "source": [
    "# check the table's name\n",
    "print(mysql_engine.table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can import mysql datasets as well as other types of datasets\n",
    "* Download the chinook.db from this [website](http://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_file = 'sqlite:///chinook.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine\n",
    "sqlite_engine = create_engine(sqlite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the engine in order to retrive data\n",
    "sqlite_conn = sqlite_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albums', 'artists', 'customers', 'employees', 'genres', 'invoice_items', 'invoices', 'media_types', 'playlist_track', 'playlists', 'sqlite_sequence', 'sqlite_stat1', 'tracks']\n"
     ]
    }
   ],
   "source": [
    "print(sqlite_engine.table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Import the users table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the string to be queried\n",
    "status_user = 'SELECT * FROM stats.users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/pymysql/connections.py\", line 713, in _write_bytes\n",
      "    self._sock.sendall(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 680, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 867, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/sqlalchemy/dialects/mysql/base.py\", line 2241, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/pymysql/connections.py\", line 429, in rollback\n",
      "    self._execute_command(COMMAND.COM_QUERY, \"ROLLBACK\")\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/pymysql/connections.py\", line 771, in _execute_command\n",
      "    self._write_bytes(packet)\n",
      "  File \"/Users/liviaclarete/.pyenv/versions/dataAnalysis/lib/python3.6/site-packages/pymysql/connections.py\", line 718, in _write_bytes\n",
      "    \"MySQL server has gone away (%r)\" % (e,))\n",
      "pymysql.err.OperationalError: (2006, \"MySQL server has gone away (BrokenPipeError(32, 'Broken pipe'))\")\n"
     ]
    }
   ],
   "source": [
    "# Query the dataset using pandas\n",
    "user = pd.read_sql_query(status_user,\n",
    "                        mysql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Rename Id column to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Import the posts table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rename Id column to postId and OwnerUserId to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Define new dataframes for users and posts with the following selected columns:\n",
    "    **users columns**: userId, Reputation,Views,UpVotes,DownVotes\n",
    "    **posts columns**: postId, Score,userID,ViewCount,CommentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Merge both dataframes, users and posts. \n",
    "You will need to make a [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) of posts and users dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. How many missing values do you have in your merged dataframe? On which columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. You will need to make something with missing values.  Will you clean or filling them? Explain. \n",
    "**Remember** to check the results of your code before passing to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Adjust the data types in order to avoid future issues. Which ones should be changed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Identify extreme values in your merged dataframe as you have learned in class, create a dataframe called outliers with the same columns as our data set and calculate the bounds. The values of the outliers dataframe will be the values of the merged_df that fall outside that bounds. You will need to save your outliers dataframe to a csv file on your-code folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
